{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQXNkM7zT9pNYrVv/xnOoA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiru1997/AI-agent/blob/main/chapter_20_LLM_loss_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5UsNO2nAEDo0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"n_positions\": 1024,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_layers\": 12,\n",
        "    \"n_heads\": 12,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": False\n",
        "}"
      ],
      "metadata": {
        "id": "Bw5WF3YxEaxe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift= nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(-1, keepdim=True)\n",
        "    var = x.var(-1, keepdim=True, unbiased=False)\n",
        "    norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "    return self.scale * norm_x + self.shift"
      ],
      "metadata": {
        "id": "Jf7fwpUWFUi4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "YMyjOARNHtG9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Convert the constant to a tensor before applying sqrt\n",
        "    sqrt_term = torch.sqrt(torch.tensor(2.0 / torch.pi))\n",
        "    return 0.5 * x * (1 + torch.tanh(sqrt_term * (x + 0.044715 * torch.pow(x, 3))))"
      ],
      "metadata": {
        "id": "_wTtrdV6IdT0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "        GELU(),\n",
        "        nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "O_CKx_SjIaX-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.att = MultiHeadAttention(\n",
        "        d_in = cfg[\"emb_dim\"],\n",
        "        d_out = cfg[\"emb_dim\"],\n",
        "        context_length = cfg[\"n_positions\"],\n",
        "        num_heads = cfg[\"n_heads\"],\n",
        "        qkv_bias = cfg[\"qkv_bias\"],\n",
        "        dropout = cfg[\"drop_rate\"]\n",
        "    )\n",
        "    self.ff = FeedForward(cfg)\n",
        "    self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "  def forward(self, x):\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x = self.att(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = shortcut + x\n",
        "\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = shortcut + x\n",
        "    return x"
      ],
      "metadata": {
        "id": "SXJ5NZCwHp8P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
        "    self.pos_emb = nn.Embedding(config[\"n_positions\"], config[\"emb_dim\"])\n",
        "    self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "    self.trf_blocks = nn.Sequential(*[TransformerBlock(config) for _ in range(config[\"n_layers\"])])\n",
        "    self.final_norm = LayerNorm(config[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, seq_length = x.shape\n",
        "    tok_emb = self.tok_emb(x)\n",
        "    pos_emb = self.pos_emb(torch.arange(seq_length, device=x.device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "YQAv8CbNHJUH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "batch = [[6109, 3626, 6100, 345], [6109, 1110, 6622, 257]]\n",
        "out = model(torch.tensor(batch))\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWAVh7ErIfln",
        "outputId": "97c9d20d-ee21-46e3-a25b-4a70d0b0df18"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0269, -0.1110,  0.6107,  ...,  0.1151,  0.1747, -0.0079],\n",
            "         [ 0.7574, -0.0585, -0.5831,  ..., -0.3706, -0.3201, -0.2644],\n",
            "         [-0.7286, -0.9251, -0.0859,  ...,  0.4201,  0.7133,  0.0559],\n",
            "         [-1.1222,  0.8962,  0.8873,  ...,  0.6075,  0.3478,  0.1706]],\n",
            "\n",
            "        [[-0.1231,  0.2811,  0.2888,  ..., -0.2556,  0.3092, -0.3788],\n",
            "         [ 0.2744, -0.0373, -0.2586,  ...,  0.1978, -0.2758, -0.1446],\n",
            "         [ 0.4581, -0.2585,  0.6123,  ...,  0.6033,  0.7874,  0.5151],\n",
            "         [ 0.1448,  0.4149,  1.0251,  ..., -0.1461, -0.3394, -0.2725]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXNZD8RNQo7z",
        "outputId": "ae80b508-38e2-4ab0-dd17-417001f7ae43"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 163009536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " ! pip3 install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4yPAJPUapa3",
        "outputId": "64797a41-0f3b-426d-9029-726a3856399b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "    [\n",
        "        [16833, 3626, 6100],\n",
        "        [40, 1107, 588]\n",
        "    ]\n",
        ")\n",
        "\n",
        "targets = torch.tensor(\n",
        "    [\n",
        "        [3626, 6100, 345],\n",
        "        [1107, 588, 11311]\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "2y0isPGQvc15"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    logits = model(inputs)\n",
        "\n",
        "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
        "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IyYFALcJsVn",
        "outputId": "ab102bba-a1da-4420-9d61-2e50a10171d5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = torch.argmax(probas, dim=-1,keepdim=True)\n",
        "print(token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmHR05nPKywI",
        "outputId": "4748ecc8-bfb8-46fa-a9b6-3eb1f5df13fe"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[30470],\n",
            "         [ 3642],\n",
            "         [46216]],\n",
            "\n",
            "        [[25897],\n",
            "         [ 5391],\n",
            "         [ 2131]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_id = 0\n",
        "target_probas_1 = probas[text_id,  [0, 1, 2] , targets[text_id]]\n",
        "print(target_probas_1)\n",
        "\n",
        "text_id = 1\n",
        "target_probas_2 = probas[text_id,  [0, 1, 2], targets[text_id]]\n",
        "print(target_probas_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN8TysatLYlN",
        "outputId": "0a34c6d5-6aea-4593-eee1-704c3061aad3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.0339e-05, 2.5892e-05, 1.1982e-05])\n",
            "tensor([3.2835e-05, 4.2851e-06, 2.2142e-05])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_probs = torch.log(torch.cat([target_probas_1, target_probas_2]))\n",
        "print(log_probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0azNSv4nYvsW",
        "outputId": "788b18e5-55a8-4f3f-a908-c1e74ae93f96"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-11.4796, -10.5616, -11.3321, -10.3240, -12.3604, -10.7180])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_log_probas = torch.mean(log_probs)\n",
        "print(avg_log_probas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzpazPW5ob2J",
        "outputId": "493556f6-6588-4905-81ca-91a4afe582f1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-11.1293)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neg_avg_log_probas = avg_log_probas * -1\n",
        "print(neg_avg_log_probas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0l97LrPoxnz",
        "outputId": "86ea7961-daac-4358-9fc2-0dfd6023a741"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(11.1293)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(logits)\n",
        "\n",
        "logits_flat = logits.flatten(0, 1)\n",
        "target_flat = targets.flatten()\n",
        "print(logits_flat.shape)\n",
        "print(target_flat.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9UJm_eHpA6t",
        "outputId": "1cc00dd5-f8bc-4433-9e35-3d78eb07b76f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.3704, -0.8010,  0.4840,  ..., -0.3238, -0.5666, -0.6468],\n",
            "         [ 1.1628,  0.0099, -0.7192,  ...,  0.1731, -0.6708, -0.4625],\n",
            "         [-0.1887, -1.0521, -0.4653,  ...,  0.3827,  0.4168,  0.0790]],\n",
            "\n",
            "        [[-0.4195, -0.4706, -0.5642,  ...,  0.1788, -0.3350, -0.1288],\n",
            "         [ 0.2063, -0.5483, -0.7196,  ..., -0.1612, -0.2216,  0.2542],\n",
            "         [-0.1132, -0.7935, -0.9667,  ...,  0.5666,  0.7748,  0.5278]]])\n",
            "torch.Size([6, 50257])\n",
            "torch.Size([6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(logits_flat)\n",
        "print(target_flat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yxy5870Xp9op",
        "outputId": "2df25714-35fc-4ec7-b8b0-105db6c63ce1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.3704, -0.8010,  0.4840,  ..., -0.3238, -0.5666, -0.6468],\n",
            "        [ 1.1628,  0.0099, -0.7192,  ...,  0.1731, -0.6708, -0.4625],\n",
            "        [-0.1887, -1.0521, -0.4653,  ...,  0.3827,  0.4168,  0.0790],\n",
            "        [-0.4195, -0.4706, -0.5642,  ...,  0.1788, -0.3350, -0.1288],\n",
            "        [ 0.2063, -0.5483, -0.7196,  ..., -0.1612, -0.2216,  0.2542],\n",
            "        [-0.1132, -0.7935, -0.9667,  ...,  0.5666,  0.7748,  0.5278]])\n",
            "tensor([ 3626,  6100,   345,  1107,   588, 11311])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`torch.nn.functional.cross_entropy` is a function in PyTorch used to compute the **cross-entropy loss** between logits and ground-truth class indices for classification tasks. It combines `log_softmax` and `nll_loss` in one function, making it numerically more stable.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”§ **Function Signature**\n",
        "\n",
        "```python\n",
        "torch.nn.functional.cross_entropy(input, target, weight=None, size_average=None,\n",
        "                                  ignore_index=-100, reduce=None, reduction='mean',\n",
        "                                  label_smoothing=0.0)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“¥ **Arguments**\n",
        "\n",
        "| Argument          | Description                                                                       |\n",
        "| ----------------- | --------------------------------------------------------------------------------- |\n",
        "| `input`           | Tensor of shape **(N, C, ...)** â€“ raw scores (logits) for each class.             |\n",
        "| `target`          | Tensor of shape **(N, ...)** â€“ ground-truth class indices (0 â‰¤ target\\[i] < C).   |\n",
        "| `weight`          | Optional tensor of shape **(C,)** â€“ weight per class.                             |\n",
        "| `ignore_index`    | Specifies a target value that is **ignored** and does not contribute to the loss. |\n",
        "| `reduction`       | `'none'`, `'mean'` (default), or `'sum'`.                                         |\n",
        "| `label_smoothing` | Float for label smoothing (default: 0.0).                                         |\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… **Key Notes**\n",
        "\n",
        "* **Input shape**: The logits **must not be passed through softmax** before calling this function.\n",
        "* **Target** must contain integer class labels, **not one-hot**.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Œ **Example Usage**\n",
        "\n",
        "### Binary Classification (as Multi-Class)\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 2 samples, 3 classes (logits)\n",
        "logits = torch.tensor([[1.2, 0.3, 2.1],\n",
        "                       [0.1, 2.4, 0.5]])\n",
        "\n",
        "# Ground truth class indices\n",
        "targets = torch.tensor([2, 1])\n",
        "\n",
        "loss = F.cross_entropy(logits, targets)\n",
        "print(loss)  # scalar loss value\n",
        "```\n",
        "\n",
        "### With `reduction='none'`\n",
        "\n",
        "```python\n",
        "loss = F.cross_entropy(logits, targets, reduction='none')\n",
        "print(loss)  # tensor of shape (N,)\n",
        "```\n",
        "\n",
        "### With Class Weights\n",
        "\n",
        "```python\n",
        "weights = torch.tensor([1.0, 2.0, 0.5])  # weight class 1 higher, class 2 lower\n",
        "loss = F.cross_entropy(logits, targets, weight=weights)\n",
        "```\n",
        "\n",
        "### With Ignore Index\n",
        "\n",
        "```python\n",
        "targets = torch.tensor([2, -100])  # second target ignored\n",
        "loss = F.cross_entropy(logits, targets, ignore_index=-100)\n",
        "```\n",
        "\n",
        "### With Label Smoothing\n",
        "\n",
        "```python\n",
        "loss = F.cross_entropy(logits, targets, label_smoothing=0.1)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Intuition\n",
        "\n",
        "For a single sample:\n",
        "\n",
        "$$\n",
        "\\text{Loss} = -\\log \\left( \\frac{e^{\\text{logit}_{\\text{true class}}}}{\\sum_{j} e^{\\text{logit}_j}} \\right)\n",
        "$$\n",
        "\n",
        "So it penalizes the model when the predicted probability for the correct class is low.\n",
        "\n",
        "---\n",
        "\n",
        "Let me know if you'd like a breakdown of the math, or how to use it in a training loop.\n"
      ],
      "metadata": {
        "id": "y67sC4ANwIwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = torch.nn.functional.cross_entropy(logits_flat, target_flat)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElnRJRE6qYjk",
        "outputId": "526cc724-0c82-4f80-d323-128870f8c27c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(11.1293)\n"
          ]
        }
      ]
    }
  ]
}